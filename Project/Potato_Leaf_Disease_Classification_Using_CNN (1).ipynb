{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZA4iM1ivmC3M"
      },
      "source": [
        "# **Potato Leaf Disease Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUtgZeXPmC3U"
      },
      "source": [
        "# **Import all the Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5KM_xodmC3V",
        "outputId": "969f8d2a-b058-4d30-dcb7-b9f47f0b673b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers, Sequential, models\n",
        "import pathlib\n",
        "\n",
        "from tensorflow.python.client import device_lib \n",
        "print(device_lib.list_local_devices())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwO8GUgTmC3W"
      },
      "source": [
        "### Set all the Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnwqPlEzmC3X"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "IMAGE_SIZE = 256\n",
        "CHANNELS=3\n",
        "EPOCHS=20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNatMe460KYZ",
        "outputId": "0a324dfc-ede7-4a19-aa35-3a20d0b337c3"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJaI8UyRmC3Y"
      },
      "source": [
        "# **Import data into tensorflow dataset object**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeNfzXxRmC3Z",
        "outputId": "3a463b9e-9281-4030-d857-adca68b5fe86"
      },
      "outputs": [],
      "source": [
        "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"/content/drive/MyDrive/content/drive/Training\",\n",
        "    seed=12,\n",
        "    shuffle=True,\n",
        "    image_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewKqXvBVmC3d",
        "outputId": "df6375fa-4ca5-4746-f9d3-c145461d67d0"
      },
      "outputs": [],
      "source": [
        "class_names = dataset.class_names\n",
        "class_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrx7sEX3mC3f"
      },
      "source": [
        "# **Visualize some of the images from our dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CEqSMC-6mC3f",
        "outputId": "ea0f6aa7-7de7-4703-f01f-660489603f59"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 20))\n",
        "for image_batch, labels_batch in dataset.take(1):\n",
        "    for i in range(12):\n",
        "        ax = plt.subplot(3, 4, i + 1)\n",
        "        plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n",
        "        plt.title(class_names[labels_batch[i]])\n",
        "        plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfNRLII0_-S3"
      },
      "source": [
        "# **Train, Test, validation data split**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z10Vff3TmC3k"
      },
      "outputs": [],
      "source": [
        "def get_dataset_partitions_tf(ds, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n",
        "    assert (train_split + test_split + val_split) == 1\n",
        "    \n",
        "    ds_size = len(ds)\n",
        "    \n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(shuffle_size, seed=12)\n",
        "    \n",
        "    train_size = int(train_split * ds_size)\n",
        "    val_size = int(val_split * ds_size)\n",
        "    \n",
        "    train_ds = ds.take(train_size)    \n",
        "    val_ds = ds.skip(train_size).take(val_size)\n",
        "    test_ds = ds.skip(train_size).skip(val_size)\n",
        "    \n",
        "    return train_ds, val_ds, test_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkuNq6SFmC3k"
      },
      "outputs": [],
      "source": [
        "train_ds, val_ds, test_ds = get_dataset_partitions_tf(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBWIoMo7mC3m",
        "outputId": "3816e9fd-9151-49fa-d182-8007f6fb80a9"
      },
      "outputs": [],
      "source": [
        "print(\"Size of Data is :{0} \\nBatch size of Training Data is :{1}\\nBatch size of Validation Data is :{2} \\nBatch size of Testing Data is :{3} \" .format(len(dataset), len(train_ds), len(val_ds), len(test_ds)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeggX0SNmC3m"
      },
      "source": [
        "### Cache, Shuffle, and Prefetch the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpBQFoImmC3m"
      },
      "outputs": [],
      "source": [
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKH1q3jimC3m"
      },
      "source": [
        "## Building the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeOVLwUFmC3n"
      },
      "source": [
        "### Creating a Layer for Resizing and Normalization\n",
        "Before we feed our images to network, we should be resizing it to the desired size. \n",
        "Moreover, to improve model performance, we should normalize the image pixel value (keeping them in range 0 and 1 by dividing by 256).\n",
        "This should happen while training as well as inference. Hence we can add that as a layer in our Sequential Model.\n",
        "\n",
        "You might be thinking why do we need to resize (256,256) image to again (256,256). You are right we don't need to but this will be useful when we are done with the training and start using the model for predictions. At that time somone can supply an image that is not (256,256) and this layer will resize it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yNCUfMEmC3n"
      },
      "outputs": [],
      "source": [
        "resize_and_rescale = tf.keras.Sequential([\n",
        "  layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
        "  layers.Rescaling(1./255),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVctpKNCmC3n"
      },
      "source": [
        "### Data Augmentation\n",
        "Data Augmentation is needed when we have less data, this boosts the accuracy of our model by augmenting the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpZQAdPSmC3o"
      },
      "outputs": [],
      "source": [
        "data_augmentation = tf.keras.Sequential([\n",
        "  layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.RandomRotation(0.2),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YbAG2WvmC3o"
      },
      "source": [
        "#### Applying Data Augmentation to Train Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32L36HYgmC3o"
      },
      "outputs": [],
      "source": [
        "train_ds = train_ds.map(\n",
        "    lambda x, y: (data_augmentation(x, training=True), y)\n",
        ").prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9UYhBdWmC3p"
      },
      "source": [
        "# **Model Architecture**\n",
        "We use a CNN coupled with a Softmax activation in the output layer. We also add the initial layers for resizing, normalization and Data Augmentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mv0l0doemC3q"
      },
      "outputs": [],
      "source": [
        "input_shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n",
        "n_classes = 3\n",
        "\n",
        "model = models.Sequential([\n",
        "    resize_and_rescale,\n",
        "    layers.Conv2D(32, kernel_size = (3,3), activation='relu', input_shape=input_shape),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(n_classes, activation='softmax'),\n",
        "])\n",
        "\n",
        "model.build(input_shape=input_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxlTjsYmmC3q",
        "outputId": "5b789d97-25ff-4c8a-aa40-a05e983acd52"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqJUXSgBmC3r"
      },
      "source": [
        "# **Compiling the Model**\n",
        "We use `adam` Optimizer, `SparseCategoricalCrossentropy` for losses, `accuracy` as a metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGODBMvmmC3r"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78jrT6gpCCyU"
      },
      "source": [
        "# **Training The Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CD3WZTgmC3r",
        "outputId": "277aebf6-ed1a-4bcf-8858-9caaa2a5c0e0",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=val_ds,\n",
        "    verbose=1,\n",
        "    epochs=EPOCHS,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqEDrAzOmC3r",
        "outputId": "9d63f665-176d-4620-a7a2-cbcab0134b18"
      },
      "outputs": [],
      "source": [
        "scores = model.evaluate(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQWOkWxvmC3s"
      },
      "source": [
        "Scores is just a list containing loss and accuracy value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90KsTSzVmC3s"
      },
      "source": [
        "# **Plotting the Accuracy and Loss Curves**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5vuSEbemC3v"
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "id": "QtPsyX7QmC3v",
        "outputId": "b68c3da4-bae3-44e8-a832-0bcb9cb2b18f"
      },
      "outputs": [],
      "source": [
        "#graphs for accuracy and loss of training and validation data\n",
        "plt.figure(figsize = (20,20))\n",
        "plt.subplot(2,3,1)\n",
        "plt.plot(range(EPOCHS), acc, label = 'Training Accuracy')\n",
        "plt.plot(range(EPOCHS), val_acc, label = 'Validation Accuracy')\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.title('Training and Validation Accuracy') \n",
        "\n",
        "plt.subplot(2,3,2)\n",
        "plt.plot(range(EPOCHS), loss, label = 'Training Loss')\n",
        "plt.plot(range(EPOCHS), val_loss, label = 'Validation Loss')\n",
        "plt.legend(loc = 'upper right')\n",
        "plt.title('Training and Validation Loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtAKhDl6mC3v"
      },
      "source": [
        "# **Running prediction on a sample images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "C9rYp3egmC3v",
        "outputId": "0559e024-8137-432f-a397-08d7e5655195",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "for images_batch, labels_batch in test_ds.take(1):\n",
        "    \n",
        "    first_image = images_batch[0].numpy().astype('uint8')\n",
        "    first_label = labels_batch[0].numpy()\n",
        "    \n",
        "    print(\"first image to predict\")\n",
        "    plt.imshow(first_image)\n",
        "    print(\"actual label:\",class_names[first_label])\n",
        "    \n",
        "    batch_prediction = model.predict(images_batch)\n",
        "    print(\"predicted label:\",class_names[np.argmax(batch_prediction[0])])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ1ngJDBmC3w"
      },
      "source": [
        "# **Function to test for inference**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kag9vgmWmC3w"
      },
      "outputs": [],
      "source": [
        "def predict(model, img):\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(images[i].numpy())\n",
        "    img_array = tf.expand_dims(img_array, 0)\n",
        "\n",
        "    predictions = model.predict(img_array)\n",
        "    print(predictions)\n",
        "\n",
        "    predicted_class = class_names[np.argmax(predictions[0])]\n",
        "    confidence = round(100 * (np.max(predictions[0])), 2)\n",
        "    return predicted_class, confidence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daiNir1_mC3w"
      },
      "source": [
        "# **Now running inference on few sample images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ts2CJCGemC3w",
        "outputId": "62529cb4-96b9-4e9b-9bac-8a37f8948ad3"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 20))\n",
        "for images, labels in test_ds.take(1):\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        \n",
        "        predicted_class, confidence = predict(model, images[i].numpy())\n",
        "        actual_class = class_names[labels[i]] \n",
        "        \n",
        "        plt.title(f\"Actual: {actual_class},\\n Predicted: {predicted_class}.\\n Confidence: {confidence}%\")\n",
        "        \n",
        "        plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB6OAulFAnlS"
      },
      "source": [
        "# **Saving the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lu0z-3AELO6"
      },
      "outputs": [],
      "source": [
        "# saved_model_path = '/content/drive/MyDrive/models/my_model'\n",
        "# save_options = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
        "# model.save(saved_model_path, options=save_options)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "3ce76490c07a75ea24134e09186af2521c358252e3ea70d858a2691e4a58fda3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
